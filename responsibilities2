Experience in analysis, design, development and migration using Bigdata Hadoop Components like Hive, Sqoop, pySpark, Kafka, HBase, AWS, Cloudera, Horton works, Oozie, Data Processing.
Good knowledge on Hadoop Architecture and its internals such as HDFS, Map Reduce, Yarn, Spark.
Extensive knowledge on Hadoop technology experience in Storage, processing and analysis of data using Pyspark DF & Spark SQL & Hive, Sqoop.
Good knowledge on transferring streaming data from different data sources into HDFS systems using Kafka.
Expertise on working with Spark SQL to read & write into various RDBMS systems.
Experience in working with different data sources like Flat files, XML, JSON files and Databases.
Functional knowledge on using job scheduling and monitoring tools like crontab, Oozie and Airflow.
Strong Experience in troubleshooting the Big Data Components and UNIX, and handling with data issues and performance tuning.
Have the motivation to take independent responsibility as well as ability to contribute and be a productive team member.
Presented multiple sessions to Business Users on Hadoop use-cases for opensource Environment.
Good understanding and experience with Software Development methodologies like Agile and Waterfall.
Expertise in using Version Control systems like GIT HUB.

 
Responsibilities:

Working on Spark Jobs were written to perform Data Quality checks on data before files were moved to Data Processing Layer from Raw Layer.

Implementing Spark using Pyspark & Scala based on data source, Spark SQL for faster testing and processing of data.

Responsible for creating data pipeline using Kafka to collect the logs from the source system to HDFS.

Ingested data from Relational DB using SQOOP into HDFS and loaded them into Hive tables and transformed and analyzed large datasets by running Hive queries and using Spark SQL.

Involved in creating Hive tables, loading with data and writing hive queries.

Handling Different databases like MySQL, RDBMS into HDFS, Hive and HBASE using Sqoop. 

Handling Pyspark (DataFrames, Spark SQL), Hive, Oozie Jobs on performance tuning, troubleshooting, and also working with Admins to tuning clusters. 

Working with UNIX/LINUX environments, writing UNIX shell scripts for Spark & Hive Jobs automation.

Working on Agile development environment in sprint cycles of two weeks by dividing and organizing tasks. Participated on daily scrum and other design related meetings. 

Work with Network, Database, Application, QA and BI teams to ensure data quality and availability.

Environment: Hadoop, HDFS, MapReduce, Yarn, Hive, Pig, HBase, Oozie, Sqoop, Kafka, Oracle 11g, Pyspark, Scala, HDP 2.6.2,Unix, Anaconda, Jupyter Notebooh, Hue, Jira, Github.



Responsibilities:

Implemented Data Interface to get information of customers using Pyspark and store into HDFS.
Extracted files from MySQL, Oracle, and Teradata through Sqoop and placed in HDFS and processed for Transfromations using Spark ETL.
Worked with various HDFS file formats like Avro, Sequence File, Json, XML and various compression formats like Snappy, bzip2,zlib.
Used spark to parse XML files and extract values from XML tags and load it into multiple hive tables.
Used Hive to perform data validation on the data ingested using sqoop and Pyspark and the cleansed data set, and pushed into Hbase on daily basis.
Used Spark for interactive queries, processing of batch data and integration with NoSQL database for huge volume of data.
Troubleshooting, debugging & altering Pyspark, Sqoop, Hive issues, while maintaining the health and performance of the ETL environment.
Used Jira for bug tracking and, GIT to check-in and checkout code changes.
Weekly meetings with technical collaborators and active participation in code review sessions with senior and junior developers using Agile methodology. 


Proficiency in writing the Unix/Linux shell for Pyspark & Sqoop jobs automation.


Environment: Hadoop, HDFS, MapReduce, Yarn, Hive, HBase, Oozie, Sqoop, Pyspark, Scala, HDP,Eclipse, Oozie, Unix/Linux, Aws, Python, Perl, Zookeeper.
 

 
Responsibilities:
 
Involved in loading data from RDBMS and web logs into HDFS using Sqoop and Flume. 

Improved performances of different Spark jobs and Hive jobs, and tuned the jobs in an efficient manner.
Performed Importing and exporting data from Oracle to HDFS and Hive using Sqoop 
Wrote Pyspark Scripts to perform ETL on the data in HDFS. 
Optimized Daily Jobs to use HDFS efficiently by using various compression mechanisms.
Analyzed the data by performing Hive queries and running Pyspark scripts to study customer behavior, and was working closely with Data Science Team. 
Analyzed the partitioned and bucketed data on Hive and compute various metrics for reporting. 
Used Oozie Scheduler system to automate the pipeline workflow and orchestrate the map reduces jobs that extract the data on a timely manner.
Good understanding on Agile & git Environment, we use it for daily basis. 
 
Environment: Hadoop, HDFS, spark, HDP Horton, Sqoop, Data Processing Layer, HUE,MS Visio, Tableau, SQL, MongoDB, Oozie, UNIX, MySQL, RDBMS, Ambari, HBase, Cron.
  

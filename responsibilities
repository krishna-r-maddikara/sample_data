Summary :

.Good technical skills in handling Hadoop ecosystem including (HDFS, MapReduce, Hive, Sqoop, Pyspark (Spark core, SparkSQL, Spark Streaming), Kafka, Sqoop,Oozie.
.Experience in Design, Develop, Test, Deploy and Support Big Data Applications on HDP & Cloudera Distribution including some of the Cloud Applicaiton (AWS).
.Hands on experience in Handling Hive tables (External table and internal table) including with partitioning (Static/dynamic) and bucketing.
.Hands on experience in batch load data between RDBMS(SQL server, Mysql, Oracle, DB2) and Hadoop HDFS/Hive using Sqoop & spark (Python, Scala).
.Good exposere on data cleansing, data transformation by using Spark RDD transformations such as Map, Reduce, Filter, Fold, ReduceByKey, CombineByKey and optimize RDD transformation tasks with DAG (Directed Acyclic Graph).
.Experience and understanding on ETL (Extract, Transform, Load) workflow from different sourses to different destinations (AWS redshift S3, Microsoft SQL server database, Oracle database, Hadoop HDFS, flat files). 
.Experience in working with various data formats such as ORC, Parqut, Avro, Sequence file, JSON, XML. 
.Familiar with Agile envoronment & SDLC Environment, and having very good exposere on using JIRA & Git, 

Responsibilities:

Responsible for ingesting huge amounts of data from multiple sources and stored in HDFS.

Involving in Creating Hive tables, dynamic partitions, buckets for sampling, and working on them using HiveQL

Handling structured and unstructured data and applying ETL processes. Experience in Data Warehousing and ETL using Spark.

Working on large datasets using Spark(Pyspark) in Memory capabilities, using broadcasts variables in Spark, effective & efficient joins, transformations. 

Using Spark SQL to load JSON data and create schema RDD and loaded it into HIVE tables and handlling structured data using spark SQL.

Setup Oozie workflows with Sqoop actions to migrate the data from relational databases like RDBMS to HDFS.

Experience in developing Data Conversion Scripts using Pre-Stage, Stage, Pre-Target and Target tables.

Hands on Experience in handling different file formats like CSV, text, Avro, ORC, Parquet, Sequence files, Xml and Json files.

Monitoring Full/Incremental/Daily Loads and support all scheduled ETL jobs for batch processing Spark, Sqoop Jobs.

Involving in various phases of Software Development Life Cycle (SDLC) of the application like Requirement gathering, Design, Analysis and Code development.

Environment: Hadoop, HDFS, HDP 2.6.4, Hive, Oozie, Zookeeper, Hbase, Spark (Pyspark), Spark SQL, NoSQL, Scala, Kafka,Tableau, git.

Responsibilities::

HIVE to do transformations event joins and some pre-aggregations before storing the data onto HDFS

The Hive tables created as per requirement were internal or external tables defined with appropriate static and dynamic partitions, intended for efficiency.

Worked with Data Warehousing Team to Convert existing SQL queries to Hive using Spark SQL.

Worked with different file formats when using Pyspark & Hive and compression techniques to determine standards.

Performance turned with Spark application using Catalyst optimizer, memory tuning customized partitioner, broadcasting variables on dailybasis data.  

Worked on shell scripts for scheduling various data cleansing scripts and ETL loading & Automation process.

Improved performances of multiple Spark jobs and Hive jobs, and tuned the jobs in an efficient manner on our Data Lake. 

Used Oozie workflow engine to manage interdependent Hadoop jobs and to automate Hadoop jobs such as Hive and Sqoop, Pyspark jobs.

Environment: HDFS, Hive,Spark2.1, kafka0.11, MySQL5.7, YARN, JIRA 6.4, Linux,Git


Responsibilities:

Responsible for gathering all required information and requirements for the project. 

Worked on Spark Data frames for ingesting data into Hive from flat files into RDD's to transform Unstructured data in Structured data.

Implemented partitioning, bucketing in Hive for better organization of the data.

Developed UDF's to implement complex transformations on Hive using Regex.

Involved in different data sources like Flat files, XML files and RDBMS Sources.

Managing and reviewing Hadoop log files based on Application ID from yarn UI to identify bugs & was helpful for enhancing code and performance tuning.

Written Shell scripts to start the use-case and for pre validations on new data.

Involved in Hadop production support from Dev team, which involved monitoring server and error logs, and foreseeing and preventing potential issues, and escalating issue when necessary.

Documented Technical Specs, Dataflow, Data Models and Class Models.

Environment: Hadoop, HDFS, MapReduce, Yarn, Hive, Pig, HBase, Oozie, Sqoop,Oracle 11g,Pyspark, Scala,Oozie, Node.js, Unix/Linux, Aws, JQuery, Ajax, Python,  Zookeeper.
